{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "cf21be50",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cf21be50",
        "outputId": "e8509326-5b15-4b87-f395-c15e569419eb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3848e35c",
      "metadata": {
        "id": "3848e35c"
      },
      "source": [
        "# CIC2018 Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "44747593",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "44747593",
        "outputId": "d1d615f4-fd04-473b-e9b7-2bf2d658831e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: catboost in /usr/local/lib/python3.11/dist-packages (1.2.8)\n",
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.11/dist-packages (2.1.4)\n",
            "Requirement already satisfied: lightgbm in /usr/local/lib/python3.11/dist-packages (4.5.0)\n",
            "Requirement already satisfied: imbalanced-learn in /usr/local/lib/python3.11/dist-packages (0.13.0)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.11/dist-packages (from catboost) (0.20.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from catboost) (3.10.0)\n",
            "Requirement already satisfied: numpy<3.0,>=1.16.0 in /usr/local/lib/python3.11/dist-packages (from catboost) (2.0.2)\n",
            "Requirement already satisfied: pandas>=0.24 in /usr/local/lib/python3.11/dist-packages (from catboost) (2.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from catboost) (1.15.3)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.11/dist-packages (from catboost) (5.24.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from catboost) (1.17.0)\n",
            "Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.11/dist-packages (from xgboost) (2.21.5)\n",
            "Requirement already satisfied: scikit-learn<2,>=1.3.2 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn) (1.6.1)\n",
            "Requirement already satisfied: sklearn-compat<1,>=0.1 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn) (0.1.3)\n",
            "Requirement already satisfied: joblib<2,>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn) (1.5.0)\n",
            "Requirement already satisfied: threadpoolctl<4,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn) (3.6.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.24->catboost) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.24->catboost) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.24->catboost) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (4.58.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (3.2.3)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from plotly->catboost) (9.1.2)\n",
            "Path to dataset files: /kaggle/input/csecicids2018\n",
            "Label\n",
            "Benign                   743498\n",
            "DoS attacks-GoldenEye     41406\n",
            "DoS attacks-Slowloris      9908\n",
            "Name: count, dtype: int64\n",
            "Label\n",
            "Benign    743498\n",
            "DoS        51314\n",
            "Name: count, dtype: int64\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-9-b39abb09e15f>:40: FutureWarning: The behavior of Series.replace (and DataFrame.replace) with CategoricalDtype is deprecated. In a future version, replace will only be used for cases that preserve the categories. To change the categories, use ser.cat.rename_categories instead.\n",
            "  cic['Label'] = cic['Label'].replace(dos_labels, 'DoS')\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üéØ Random Forest Test Accuracy: 0.9998\n",
            "\n",
            "üîç Classification Report for Random Forest:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     Class 0       1.00      1.00      1.00     10000\n",
            "     Class 1       1.00      1.00      1.00     10000\n",
            "\n",
            "    accuracy                           1.00     20000\n",
            "   macro avg       1.00      1.00      1.00     20000\n",
            "weighted avg       1.00      1.00      1.00     20000\n",
            "\n",
            "üéØ Gradient Boosting Test Accuracy: 0.9990\n",
            "\n",
            "üîç Classification Report for Gradient Boosting:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     Class 0       1.00      1.00      1.00     10000\n",
            "     Class 1       1.00      1.00      1.00     10000\n",
            "\n",
            "    accuracy                           1.00     20000\n",
            "   macro avg       1.00      1.00      1.00     20000\n",
            "weighted avg       1.00      1.00      1.00     20000\n",
            "\n",
            "üéØ XGBoost Test Accuracy: 0.9996\n",
            "\n",
            "üîç Classification Report for XGBoost:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     Class 0       1.00      1.00      1.00     10000\n",
            "     Class 1       1.00      1.00      1.00     10000\n",
            "\n",
            "    accuracy                           1.00     20000\n",
            "   macro avg       1.00      1.00      1.00     20000\n",
            "weighted avg       1.00      1.00      1.00     20000\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üéØ LightGBM Test Accuracy: 0.9999\n",
            "\n",
            "üîç Classification Report for LightGBM:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     Class 0       1.00      1.00      1.00     10000\n",
            "     Class 1       1.00      1.00      1.00     10000\n",
            "\n",
            "    accuracy                           1.00     20000\n",
            "   macro avg       1.00      1.00      1.00     20000\n",
            "weighted avg       1.00      1.00      1.00     20000\n",
            "\n",
            "0:\tlearn: 0.5895582\ttotal: 13.9ms\tremaining: 9.69s\n",
            "100:\tlearn: 0.0009199\ttotal: 3.13s\tremaining: 18.6s\n",
            "200:\tlearn: 0.0005905\ttotal: 4.62s\tremaining: 11.5s\n",
            "300:\tlearn: 0.0004748\ttotal: 7.87s\tremaining: 10.4s\n",
            "400:\tlearn: 0.0004356\ttotal: 9.1s\tremaining: 6.79s\n",
            "500:\tlearn: 0.0004039\ttotal: 9.98s\tremaining: 3.97s\n",
            "600:\tlearn: 0.0003745\ttotal: 10.9s\tremaining: 1.79s\n",
            "699:\tlearn: 0.0003629\ttotal: 11.7s\tremaining: 0us\n",
            "üéØ CatBoost Test Accuracy: 0.9999\n",
            "\n",
            "üîç Classification Report for CatBoost:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     Class 0       1.00      1.00      1.00     10000\n",
            "     Class 1       1.00      1.00      1.00     10000\n",
            "\n",
            "    accuracy                           1.00     20000\n",
            "   macro avg       1.00      1.00      1.00     20000\n",
            "weighted avg       1.00      1.00      1.00     20000\n",
            "\n",
            "üéØ Logistic Regression Test Accuracy: 0.9958\n",
            "\n",
            "üîç Classification Report for Logistic Regression:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     Class 0       1.00      0.99      1.00     10000\n",
            "     Class 1       0.99      1.00      1.00     10000\n",
            "\n",
            "    accuracy                           1.00     20000\n",
            "   macro avg       1.00      1.00      1.00     20000\n",
            "weighted avg       1.00      1.00      1.00     20000\n",
            "\n",
            "üéØ KNN Test Accuracy: 0.9994\n",
            "\n",
            "üîç Classification Report for KNN:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     Class 0       1.00      1.00      1.00     10000\n",
            "     Class 1       1.00      1.00      1.00     10000\n",
            "\n",
            "    accuracy                           1.00     20000\n",
            "   macro avg       1.00      1.00      1.00     20000\n",
            "weighted avg       1.00      1.00      1.00     20000\n",
            "\n",
            "üéØ Na√Øve Bayes Test Accuracy: 0.8104\n",
            "\n",
            "üîç Classification Report for Na√Øve Bayes:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     Class 0       0.99      0.63      0.77     10000\n",
            "     Class 1       0.73      0.99      0.84     10000\n",
            "\n",
            "    accuracy                           0.81     20000\n",
            "   macro avg       0.86      0.81      0.80     20000\n",
            "weighted avg       0.86      0.81      0.80     20000\n",
            "\n",
            "üéØ SVM Test Accuracy: 0.9964\n",
            "\n",
            "üîç Classification Report for SVM:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     Class 0       1.00      0.99      1.00     10000\n",
            "     Class 1       0.99      1.00      1.00     10000\n",
            "\n",
            "    accuracy                           1.00     20000\n",
            "   macro avg       1.00      1.00      1.00     20000\n",
            "weighted avg       1.00      1.00      1.00     20000\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "['saved_models/feature_columns.joblib']"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "!pip install catboost xgboost lightgbm imbalanced-learn\n",
        "\n",
        "import pandas as pd\n",
        "import os\n",
        "import joblib\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from lightgbm import LGBMClassifier\n",
        "from catboost import CatBoostClassifier\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "# Install necessary packages (run only once per session)\n",
        "\n",
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"dhoogla/csecicids2018\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)\n",
        "\n",
        "# Read the Parquet file\n",
        "file_path = '/kaggle/input/csecicids2018/DoS1-Thursday-15-02-2018_TrafficForML_CICFlowMeter.parquet'\n",
        "cic = pd.read_parquet(path + \"/DoS1-Thursday-15-02-2018_TrafficForML_CICFlowMeter.parquet\")\n",
        "\n",
        "print(cic['Label'].value_counts())\n",
        "\n",
        "# Define which labels are considered DoS attacks\n",
        "dos_labels = ['DoS attacks-GoldenEye', 'DoS attacks-Slowloris']\n",
        "\n",
        "# Replace them with a unified label \"DoS\"\n",
        "cic['Label'] = cic['Label'].replace(dos_labels, 'DoS')\n",
        "print(cic['Label'].value_counts())\n",
        "\n",
        "benign_df = cic[cic['Label'] == 'Benign'].sample(n=50000, random_state=42)\n",
        "dos_df = cic[cic['Label'] == 'DoS'].sample(n=50000, random_state=42)\n",
        "\n",
        "cic = pd.concat([benign_df, dos_df], ignore_index=True)\n",
        "\n",
        "\n",
        "feature_mapping = {\n",
        "    'Total Fwd Packet': 'Total Fwd Packets',\n",
        "    'Total Bwd packets': 'Total Backward Packets',\n",
        "    'Total Length of Fwd Packet': 'Fwd Packets Length Total',\n",
        "    'CWR Flag Count': 'CWE Flag Count',\n",
        "    'Fwd Packet/Bulk Avg': 'Fwd Avg Packets/Bulk',  # Note: Duplicate key issue below\n",
        "    'Fwd Bytes/Bulk Avg': 'Fwd Avg Bytes/Bulk',\n",
        "    'Fwd Bulk Rate Avg': 'Fwd Avg Bulk Rate',\n",
        "    'Bwd Bytes/Bulk Avg': 'Bwd Avg Bytes/Bulk',\n",
        "    'Bwd Bulk Rate Avg': 'Bwd Avg Bulk Rate',\n",
        "    'Bwd Init Win Bytes': 'Init Bwd Win Bytes',\n",
        "    'Bwd Packet/Bulk Avg':'Bwd Avg Packets/Bulk',\n",
        "    'FWD Init Win Bytes': 'Init Fwd Win Bytes',\n",
        "    'Fwd Act Data Pkts': 'Fwd Act Data Packets'\n",
        "}\n",
        "reversed_feature_mapping = {v: k for k, v in feature_mapping.items()}\n",
        "\n",
        "cic = cic.rename(columns=reversed_feature_mapping)\n",
        "cic['Label'].value_counts()\n",
        "\n",
        "labels = cic['Label']\n",
        "numeric_df = cic.select_dtypes(include=[np.number])\n",
        "\n",
        "# Encode labels to binary\n",
        "le = LabelEncoder()\n",
        "y = le.fit_transform(labels)\n",
        "\n",
        "# Copy numeric features\n",
        "X = numeric_df.copy()\n",
        "\n",
        "# remove highly correlated features\n",
        "corr_matrix = X.corr().abs()\n",
        "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
        "to_drop_corr = [col for col in upper.columns if any(upper[col] > 0.95)]\n",
        "X.drop(columns=to_drop_corr, inplace=True)\n",
        "\n",
        "X.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "X.dropna(inplace=True)\n",
        "y = y[X.index]  # align y\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_scaled, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "fast_svm = CalibratedClassifierCV(\n",
        "    estimator=LinearSVC(C=0.1, max_iter=10000, random_state=RANDOM_STATE),\n",
        "    method='sigmoid',\n",
        "    cv=5\n",
        ")\n",
        "\n",
        "models = {\n",
        "    \"Random Forest\":RandomForestClassifier(\n",
        "        n_estimators=500, max_depth=None, min_samples_split=5,\n",
        "        min_samples_leaf=2, max_features='sqrt', n_jobs=-1,\n",
        "        random_state=RANDOM_STATE\n",
        "    ),\n",
        "\n",
        "    \"Gradient Boosting\":GradientBoostingClassifier(\n",
        "        n_estimators=100, learning_rate=0.05, max_depth=3,\n",
        "        subsample=0.8, random_state=RANDOM_STATE\n",
        "    ),\n",
        "\n",
        "    \"XGBoost\":XGBClassifier(\n",
        "        objective='binary:logistic',\n",
        "        eval_metric='logloss', random_state=RANDOM_STATE,\n",
        "        n_estimators=500, max_depth=6, learning_rate=0.03,\n",
        "        subsample=0.85, colsample_bytree=0.85,\n",
        "        reg_alpha=0.5, reg_lambda=2.0, n_jobs=-1,\n",
        "        tree_method='gpu_hist' if USE_GPU else 'hist',\n",
        "        verbosity=0\n",
        "    ),\n",
        "\n",
        "    \"LightGBM\":LGBMClassifier(\n",
        "        objective='binary',\n",
        "        random_state=RANDOM_STATE, n_estimators=500, learning_rate=0.03,\n",
        "        num_leaves=64, max_depth=-1, min_child_samples=20,\n",
        "        subsample=0.9, colsample_bytree=0.9,\n",
        "        reg_alpha=0.5, reg_lambda=1.0, boosting_type='gbdt',\n",
        "        n_jobs=-1, verbose=-1, device='gpu' if USE_GPU else 'cpu'\n",
        "    ),\n",
        "\n",
        "    \"CatBoost\":CatBoostClassifier(\n",
        "        iterations=700, depth=8, learning_rate=0.03,\n",
        "        task_type='GPU' if USE_GPU else 'CPU',\n",
        "        thread_count=-1, random_seed=RANDOM_STATE,\n",
        "        l2_leaf_reg=3.0, verbose=100,\n",
        "        od_type='Iter', od_wait=50,\n",
        "        loss_function='Logloss'\n",
        "    ),\n",
        "\n",
        "    \"Logistic Regression\":LogisticRegression(\n",
        "        C=0.01, penalty='l2', random_state=RANDOM_STATE,\n",
        "        max_iter=1000, n_jobs=-1\n",
        "    ),\n",
        "\n",
        "    \"KNN\":KNeighborsClassifier(\n",
        "        n_neighbors=10, n_jobs=-1\n",
        "    ),\n",
        "\n",
        "    \"Na√Øve Bayes\":GaussianNB(var_smoothing=1e-9),\n",
        "\n",
        "    \"SVM\":fast_svm,\n",
        "}\n",
        "\n",
        "for name, model in models.items():\n",
        "    # Fit on train set\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    y_proba = model.predict_proba(X_test)[:, 1] if hasattr(model, \"predict_proba\") else None\n",
        "\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "\n",
        "    print(f\"üéØ {name} Test Accuracy: {acc:.4f}\")\n",
        "    print(f\"\\nüîç Classification Report for {name}:\")\n",
        "    print(classification_report(y_test, y_pred, target_names=['Class 0', 'Class 1']))\n",
        "\n",
        "\n",
        "! rm -rf ./saved_models/\n",
        "# Save models and preprocessing objects\n",
        "save_dir = \"saved_models\"\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "feature_columns = X.columns.tolist()  # features after dropping correlated and unimportant\n",
        "\n",
        "for name, model in models.items():\n",
        "    model_path = os.path.join(save_dir, f\"{name.replace(' ', '_').lower()}.joblib\")\n",
        "    joblib.dump(model, model_path)\n",
        "\n",
        "joblib.dump(le, os.path.join(save_dir, \"label_encoder.joblib\"))\n",
        "joblib.dump(scaler, os.path.join(save_dir, \"scaler.joblib\"))\n",
        "joblib.dump(feature_columns, os.path.join(save_dir, \"feature_columns.joblib\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "30397c40",
      "metadata": {
        "id": "30397c40"
      },
      "source": [
        "## Evaluate CICIDS2018 Trained Models On CIC-UNSWB-NB15 Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "480da442",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "480da442",
        "outputId": "c097d6af-3c62-49d1-96d7-a20d781d9ff8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Label\n",
            "Benign            3450658\n",
            "Exploits            30951\n",
            "Fuzzers             29613\n",
            "Reconnaissance      16735\n",
            "Generic              4632\n",
            "DoS                  4467\n",
            "Shellcode            2102\n",
            "Backdoor              452\n",
            "Analysis              385\n",
            "Worms                 246\n",
            "Name: count, dtype: int64\n",
            "\n",
            "üîπ Evaluating on CIC UNSW-NB15 Augmented Dataset Random Forest\n",
            "‚úÖ Random Forest Accuracy: 0.5000\n",
            "\n",
            "üîç Random Forest Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      Benign       0.50      1.00      0.67      5000\n",
            "         DoS       0.00      0.00      0.00      5000\n",
            "\n",
            "    accuracy                           0.50     10000\n",
            "   macro avg       0.25      0.50      0.33     10000\n",
            "weighted avg       0.25      0.50      0.33     10000\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "üîπ Evaluating on CIC UNSW-NB15 Augmented Dataset Gradient Boosting\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Gradient Boosting Accuracy: 0.3665\n",
            "\n",
            "üîç Gradient Boosting Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      Benign       0.42      0.73      0.54      5000\n",
            "         DoS       0.00      0.00      0.00      5000\n",
            "\n",
            "    accuracy                           0.37     10000\n",
            "   macro avg       0.21      0.37      0.27     10000\n",
            "weighted avg       0.21      0.37      0.27     10000\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "üîπ Evaluating on CIC UNSW-NB15 Augmented Dataset XGBoost\n",
            "‚úÖ XGBoost Accuracy: 0.3499\n",
            "\n",
            "üîç XGBoost Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      Benign       0.41      0.70      0.52      5000\n",
            "         DoS       0.00      0.00      0.00      5000\n",
            "\n",
            "    accuracy                           0.35     10000\n",
            "   macro avg       0.21      0.35      0.26     10000\n",
            "weighted avg       0.21      0.35      0.26     10000\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "üîπ Evaluating on CIC UNSW-NB15 Augmented Dataset LightGBM\n",
            "‚úÖ LightGBM Accuracy: 0.4941\n",
            "\n",
            "üîç LightGBM Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      Benign       0.50      0.99      0.66      5000\n",
            "         DoS       0.00      0.00      0.00      5000\n",
            "\n",
            "    accuracy                           0.49     10000\n",
            "   macro avg       0.25      0.49      0.33     10000\n",
            "weighted avg       0.25      0.49      0.33     10000\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "üîπ Evaluating on CIC UNSW-NB15 Augmented Dataset CatBoost\n",
            "‚úÖ CatBoost Accuracy: 0.4695\n",
            "\n",
            "üîç CatBoost Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      Benign       0.48      0.94      0.64      5000\n",
            "         DoS       0.00      0.00      0.00      5000\n",
            "\n",
            "    accuracy                           0.47     10000\n",
            "   macro avg       0.24      0.47      0.32     10000\n",
            "weighted avg       0.24      0.47      0.32     10000\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "üîπ Evaluating on CIC UNSW-NB15 Augmented Dataset Logistic Regression\n",
            "‚úÖ Logistic Regression Accuracy: 0.1618\n",
            "\n",
            "üîç Logistic Regression Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      Benign       0.19      0.21      0.20      5000\n",
            "         DoS       0.13      0.12      0.12      5000\n",
            "\n",
            "    accuracy                           0.16     10000\n",
            "   macro avg       0.16      0.16      0.16     10000\n",
            "weighted avg       0.16      0.16      0.16     10000\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "üîπ Evaluating on CIC UNSW-NB15 Augmented Dataset KNN\n",
            "‚úÖ KNN Accuracy: 0.5391\n",
            "\n",
            "üîç KNN Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      Benign       0.52      0.99      0.68      5000\n",
            "         DoS       0.92      0.09      0.16      5000\n",
            "\n",
            "    accuracy                           0.54     10000\n",
            "   macro avg       0.72      0.54      0.42     10000\n",
            "weighted avg       0.72      0.54      0.42     10000\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "üîπ Evaluating on CIC UNSW-NB15 Augmented Dataset Na√Øve Bayes\n",
            "‚úÖ Na√Øve Bayes Accuracy: 0.5000\n",
            "\n",
            "üîç Na√Øve Bayes Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      Benign       0.50      1.00      0.67      5000\n",
            "         DoS       0.00      0.00      0.00      5000\n",
            "\n",
            "    accuracy                           0.50     10000\n",
            "   macro avg       0.25      0.50      0.33     10000\n",
            "weighted avg       0.25      0.50      0.33     10000\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "üîπ Evaluating on CIC UNSW-NB15 Augmented Dataset SVM\n",
            "‚úÖ SVM Accuracy: 0.3177\n",
            "\n",
            "üîç SVM Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      Benign       0.34      0.37      0.35      5000\n",
            "         DoS       0.30      0.26      0.28      5000\n",
            "\n",
            "    accuracy                           0.32     10000\n",
            "   macro avg       0.32      0.32      0.32     10000\n",
            "weighted avg       0.32      0.32      0.32     10000\n",
            "\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "unsw = pd.read_csv(\"/content/drive/MyDrive/Datasets/CICFlowMeter_out.csv\")\n",
        "\n",
        "\n",
        "print(unsw['Label'].value_counts())\n",
        "\n",
        "# Keep only rows where the 'Label' is 'BENIGN' or 'DoS'\n",
        "filtered_df = unsw[unsw['Label'].isin(['Benign', 'DoS'])]\n",
        "\n",
        "# Undersample BENIGN to 2000 samples\n",
        "benign_df = filtered_df[filtered_df['Label'] == 'Benign'].sample(n=5000, random_state=42)\n",
        "dos_df = filtered_df[filtered_df['Label'] == 'DoS']  # keep all DoS for now\n",
        "\n",
        "# Combine and prepare for SMOTE\n",
        "df_bal = pd.concat([benign_df, dos_df], ignore_index=True)\n",
        "\n",
        "# Separate features and labels\n",
        "X_c = df_bal.drop(columns=['Label'])\n",
        "X_c = df_bal.select_dtypes(include=[np.number])\n",
        "y_c = df_bal['Label']\n",
        "\n",
        "y_encoded = le.fit_transform(y_c)  # BENIGN = 0, DoS = 1\n",
        "\n",
        "# Apply SMOTE\n",
        "sm = SMOTE(random_state=42)\n",
        "X_res, y_res = sm.fit_resample(X_c, y_encoded)\n",
        "\n",
        "# Convert back to DataFrame\n",
        "X_res_df = pd.DataFrame(X_res, columns=X.columns)\n",
        "y_res_df = pd.DataFrame(le.inverse_transform(y_res), columns=['Label'])\n",
        "\n",
        "# Combine features and label\n",
        "final_df = pd.concat([X_res_df, y_res_df], axis=1)\n",
        "\n",
        "# 1. Separate features and labels\n",
        "final_df_X = final_df.drop(columns=['Label'])\n",
        "final_df_Y = final_df['Label']\n",
        "\n",
        "final_df_X = final_df_X[X.columns]\n",
        "\n",
        "cic2019_y_encoded = le.transform(final_df_Y)\n",
        "final_df_X_scaled = scaler.fit_transform(final_df_X)\n",
        "\n",
        "for name, model in models.items():\n",
        "    print(f\"\\nüîπ Evaluating on CIC UNSW-NB15 Augmented Dataset {name}\")\n",
        "\n",
        "    try:\n",
        "        y_pred = model.predict(final_df_X_scaled)\n",
        "        acc = accuracy_score(cic2019_y_encoded, y_pred)\n",
        "\n",
        "        print(f\"‚úÖ {name} Accuracy: {acc:.4f}\")\n",
        "        print(f\"\\nüîç {name} Classification Report:\")\n",
        "        print(classification_report(cic2019_y_encoded, y_pred, target_names=le.classes_))\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error evaluating {name}: {e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0f8792ae",
      "metadata": {
        "id": "0f8792ae"
      },
      "source": [
        "## Practical Test Of CICIDS2018 Dataset On Recorded CSVs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "d8e8873b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d8e8873b",
        "outputId": "2efd12de-7afd-495a-f5aa-580a5af6350e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "vuln dataset 1: \n",
            "üîπ Random Forest Predictions:\n",
            "  Sample 1: Benign\n",
            "  Sample 2: Benign\n",
            "  Sample 3: Benign\n",
            "  Sample 4: Benign\n",
            "  Sample 5: Benign\n",
            "  Sample 6: Benign\n",
            "----------------------------------------\n",
            "üîπ Knn Predictions:\n",
            "  Sample 1: DoS\n",
            "  Sample 2: DoS\n",
            "  Sample 3: DoS\n",
            "  Sample 4: DoS\n",
            "  Sample 5: DoS\n",
            "  Sample 6: DoS\n",
            "----------------------------------------\n",
            "üîπ Xgboost Predictions:\n",
            "  Sample 1: DoS\n",
            "  Sample 2: DoS\n",
            "  Sample 3: Benign\n",
            "  Sample 4: DoS\n",
            "  Sample 5: DoS\n",
            "  Sample 6: Benign\n",
            "----------------------------------------\n",
            "üîπ Catboost Predictions:\n",
            "  Sample 1: Benign\n",
            "  Sample 2: Benign\n",
            "  Sample 3: Benign\n",
            "  Sample 4: Benign\n",
            "  Sample 5: Benign\n",
            "  Sample 6: Benign\n",
            "----------------------------------------\n",
            "üîπ Svm Predictions:\n",
            "  Sample 1: Benign\n",
            "  Sample 2: Benign\n",
            "  Sample 3: Benign\n",
            "  Sample 4: Benign\n",
            "  Sample 5: Benign\n",
            "  Sample 6: Benign\n",
            "----------------------------------------\n",
            "üîπ Logistic Regression Predictions:\n",
            "  Sample 1: DoS\n",
            "  Sample 2: DoS\n",
            "  Sample 3: DoS\n",
            "  Sample 4: DoS\n",
            "  Sample 5: DoS\n",
            "  Sample 6: DoS\n",
            "----------------------------------------\n",
            "üîπ Lightgbm Predictions:\n",
            "  Sample 1: Benign\n",
            "  Sample 2: Benign\n",
            "  Sample 3: Benign\n",
            "  Sample 4: Benign\n",
            "  Sample 5: Benign\n",
            "  Sample 6: Benign\n",
            "----------------------------------------\n",
            "üîπ Gradient Boosting Predictions:\n",
            "  Sample 1: Benign\n",
            "  Sample 2: Benign\n",
            "  Sample 3: Benign\n",
            "  Sample 4: Benign\n",
            "  Sample 5: Benign\n",
            "  Sample 6: Benign\n",
            "----------------------------------------\n",
            "üîπ Na√Øve Bayes Predictions:\n",
            "  Sample 1: DoS\n",
            "  Sample 2: DoS\n",
            "  Sample 3: DoS\n",
            "  Sample 4: DoS\n",
            "  Sample 5: DoS\n",
            "  Sample 6: DoS\n",
            "----------------------------------------\n",
            "vuln dataset 2: \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîπ Random Forest Predictions:\n",
            "  Sample 1: Benign\n",
            "  Sample 2: Benign\n",
            "  Sample 3: Benign\n",
            "  Sample 4: Benign\n",
            "  Sample 5: Benign\n",
            "  Sample 6: Benign\n",
            "----------------------------------------\n",
            "üîπ Knn Predictions:\n",
            "  Sample 1: Benign\n",
            "  Sample 2: Benign\n",
            "  Sample 3: Benign\n",
            "  Sample 4: Benign\n",
            "  Sample 5: Benign\n",
            "  Sample 6: Benign\n",
            "----------------------------------------\n",
            "üîπ Xgboost Predictions:\n",
            "  Sample 1: Benign\n",
            "  Sample 2: Benign\n",
            "  Sample 3: Benign\n",
            "  Sample 4: Benign\n",
            "  Sample 5: Benign\n",
            "  Sample 6: Benign\n",
            "----------------------------------------\n",
            "üîπ Catboost Predictions:\n",
            "  Sample 1: Benign\n",
            "  Sample 2: Benign\n",
            "  Sample 3: Benign\n",
            "  Sample 4: Benign\n",
            "  Sample 5: Benign\n",
            "  Sample 6: Benign\n",
            "----------------------------------------\n",
            "üîπ Svm Predictions:\n",
            "  Sample 1: DoS\n",
            "  Sample 2: DoS\n",
            "  Sample 3: DoS\n",
            "  Sample 4: DoS\n",
            "  Sample 5: DoS\n",
            "  Sample 6: DoS\n",
            "----------------------------------------\n",
            "üîπ Logistic Regression Predictions:\n",
            "  Sample 1: DoS\n",
            "  Sample 2: DoS\n",
            "  Sample 3: DoS\n",
            "  Sample 4: DoS\n",
            "  Sample 5: DoS\n",
            "  Sample 6: DoS\n",
            "----------------------------------------\n",
            "üîπ Lightgbm Predictions:\n",
            "  Sample 1: Benign\n",
            "  Sample 2: Benign\n",
            "  Sample 3: Benign\n",
            "  Sample 4: Benign\n",
            "  Sample 5: Benign\n",
            "  Sample 6: Benign\n",
            "----------------------------------------\n",
            "üîπ Gradient Boosting Predictions:\n",
            "  Sample 1: Benign\n",
            "  Sample 2: Benign\n",
            "  Sample 3: Benign\n",
            "  Sample 4: Benign\n",
            "  Sample 5: Benign\n",
            "  Sample 6: Benign\n",
            "----------------------------------------\n",
            "üîπ Na√Øve Bayes Predictions:\n",
            "  Sample 1: DoS\n",
            "  Sample 2: DoS\n",
            "  Sample 3: DoS\n",
            "  Sample 4: DoS\n",
            "  Sample 5: DoS\n",
            "  Sample 6: DoS\n",
            "----------------------------------------\n",
            "normal dataset: \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîπ Random Forest Predictions:\n",
            "  Sample 1: Benign\n",
            "  Sample 2: Benign\n",
            "  Sample 3: Benign\n",
            "  Sample 4: Benign\n",
            "  Sample 5: Benign\n",
            "  Sample 6: Benign\n",
            "----------------------------------------\n",
            "üîπ Knn Predictions:\n",
            "  Sample 1: DoS\n",
            "  Sample 2: DoS\n",
            "  Sample 3: DoS\n",
            "  Sample 4: DoS\n",
            "  Sample 5: DoS\n",
            "  Sample 6: DoS\n",
            "----------------------------------------\n",
            "üîπ Xgboost Predictions:\n",
            "  Sample 1: Benign\n",
            "  Sample 2: Benign\n",
            "  Sample 3: Benign\n",
            "  Sample 4: Benign\n",
            "  Sample 5: Benign\n",
            "  Sample 6: Benign\n",
            "----------------------------------------\n",
            "üîπ Catboost Predictions:\n",
            "  Sample 1: Benign\n",
            "  Sample 2: Benign\n",
            "  Sample 3: Benign\n",
            "  Sample 4: Benign\n",
            "  Sample 5: Benign\n",
            "  Sample 6: Benign\n",
            "----------------------------------------\n",
            "üîπ Svm Predictions:\n",
            "  Sample 1: Benign\n",
            "  Sample 2: Benign\n",
            "  Sample 3: Benign\n",
            "  Sample 4: Benign\n",
            "  Sample 5: Benign\n",
            "  Sample 6: Benign\n",
            "----------------------------------------\n",
            "üîπ Logistic Regression Predictions:\n",
            "  Sample 1: DoS\n",
            "  Sample 2: DoS\n",
            "  Sample 3: DoS\n",
            "  Sample 4: DoS\n",
            "  Sample 5: DoS\n",
            "  Sample 6: DoS\n",
            "----------------------------------------\n",
            "üîπ Lightgbm Predictions:\n",
            "  Sample 1: Benign\n",
            "  Sample 2: Benign\n",
            "  Sample 3: Benign\n",
            "  Sample 4: Benign\n",
            "  Sample 5: Benign\n",
            "  Sample 6: Benign\n",
            "----------------------------------------\n",
            "üîπ Gradient Boosting Predictions:\n",
            "  Sample 1: Benign\n",
            "  Sample 2: Benign\n",
            "  Sample 3: Benign\n",
            "  Sample 4: Benign\n",
            "  Sample 5: Benign\n",
            "  Sample 6: Benign\n",
            "----------------------------------------\n",
            "üîπ Na√Øve Bayes Predictions:\n",
            "  Sample 1: DoS\n",
            "  Sample 2: DoS\n",
            "  Sample 3: DoS\n",
            "  Sample 4: DoS\n",
            "  Sample 5: DoS\n",
            "  Sample 6: DoS\n",
            "----------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "saved_models_dir = \"saved_models\"\n",
        "vuln1_csv = \"/content/drive/MyDrive/Datasets/goldeneye_ISCX.csv\"\n",
        "vuln2_csv = \"/content/drive/MyDrive/Datasets/slowloris_ISCX.csv\"\n",
        "normal_csv = \"/content/drive/MyDrive/Datasets/benign_ISCX.csv\"\n",
        "\n",
        "le = joblib.load(os.path.join(saved_models_dir, \"label_encoder.joblib\"))\n",
        "scaler = joblib.load(os.path.join(saved_models_dir, \"scaler.joblib\"))\n",
        "feature_columns = joblib.load(os.path.join(saved_models_dir, \"feature_columns.joblib\"))\n",
        "\n",
        "def testDataset(input_csv):\n",
        "  df = pd.read_csv(input_csv)\n",
        "\n",
        "  df = df[feature_columns]\n",
        "\n",
        "  df = df.apply(pd.to_numeric, errors='coerce')\n",
        "  df = df.dropna()\n",
        "\n",
        "  X_new = df.select_dtypes(include=[np.number])\n",
        "\n",
        "  X_scaled = scaler.transform(X_new)\n",
        "\n",
        "  for file in os.listdir(saved_models_dir):\n",
        "      if file.endswith(\".joblib\") and not file.startswith((\"label_encoder\", \"scaler\", \"feature_columns\")):\n",
        "          model_path = os.path.join(saved_models_dir, file)\n",
        "          model_name = file.replace(\".joblib\", \"\").replace(\"_\", \" \").title()\n",
        "\n",
        "          try:\n",
        "              model = joblib.load(model_path)\n",
        "              y_pred = model.predict(X_scaled)\n",
        "              decoded = le.inverse_transform(y_pred)\n",
        "\n",
        "              print(f\"üîπ {model_name} Predictions:\")\n",
        "              for i, pred in enumerate(decoded):\n",
        "                  print(f\"  Sample {i+1}: {pred}\")\n",
        "                  if i == 5 :\n",
        "                    break\n",
        "              print(\"-\" * 40)\n",
        "\n",
        "          except Exception as e:\n",
        "              print(f\"‚ùå Failed to predict with {model_name}: {e}\")\n",
        "\n",
        "print(\"vuln dataset 1: \")\n",
        "testDataset(vuln1_csv)\n",
        "print(\"vuln dataset 2: \")\n",
        "testDataset(vuln2_csv)\n",
        "print(\"normal dataset: \")\n",
        "testDataset(normal_csv)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dc4943a6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dc4943a6",
        "outputId": "cf4694e2-3760-4dd6-b70a-5954c58cef4e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: catboost in /usr/local/lib/python3.11/dist-packages (1.2.8)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.11/dist-packages (from catboost) (0.20.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from catboost) (3.10.0)\n",
            "Requirement already satisfied: numpy<3.0,>=1.16.0 in /usr/local/lib/python3.11/dist-packages (from catboost) (2.0.2)\n",
            "Requirement already satisfied: pandas>=0.24 in /usr/local/lib/python3.11/dist-packages (from catboost) (2.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from catboost) (1.15.3)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.11/dist-packages (from catboost) (5.24.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from catboost) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.24->catboost) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.24->catboost) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.24->catboost) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (4.58.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (3.2.3)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from plotly->catboost) (9.1.2)\n",
            "\n",
            "---- Random Forest ----\n",
            "Accuracy: 0.9693\n",
            "F1: 0.9693\n",
            "Precision: 0.9694\n",
            "Recall: 0.9693\n",
            "\n",
            "Confusion Matrix:\n",
            "       0      1\n",
            "0  13267    454\n",
            "1    388  13362\n",
            "At threshold ~0.5: FPR=0.0331, TPR=0.9719\n",
            "‚úÖ Model saved to ./models/random_forest.joblib\n",
            "\n",
            "---- Gradient Boosting ----\n",
            "Accuracy: 0.9665\n",
            "F1: 0.9665\n",
            "Precision: 0.9666\n",
            "Recall: 0.9665\n",
            "\n",
            "Confusion Matrix:\n",
            "       0      1\n",
            "0  13216    505\n",
            "1    414  13336\n",
            "At threshold ~0.5: FPR=0.0368, TPR=0.9699\n",
            "‚úÖ Model saved to ./models/gradient_boosting.joblib\n",
            "\n",
            "---- XGBoost ----\n",
            "Accuracy: 0.9685\n",
            "F1: 0.9685\n",
            "Precision: 0.9686\n",
            "Recall: 0.9685\n",
            "\n",
            "Confusion Matrix:\n",
            "       0      1\n",
            "0  13258    463\n",
            "1    401  13349\n",
            "At threshold ~0.5: FPR=0.0337, TPR=0.9708\n",
            "‚úÖ Model saved to ./models/xgboost.joblib\n",
            "\n",
            "---- LightGBM ----\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.9686\n",
            "F1: 0.9686\n",
            "Precision: 0.9686\n",
            "Recall: 0.9686\n",
            "\n",
            "Confusion Matrix:\n",
            "       0      1\n",
            "0  13255    466\n",
            "1    397  13353\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "At threshold ~0.5: FPR=0.0340, TPR=0.9709\n",
            "‚úÖ Model saved to ./models/lightgbm.joblib\n",
            "\n",
            "---- CatBoost ----\n",
            "0:\tlearn: 0.6487529\ttotal: 17.9ms\tremaining: 6.25s\n",
            "100:\tlearn: 0.1409864\ttotal: 1.01s\tremaining: 2.5s\n",
            "200:\tlearn: 0.1347293\ttotal: 1.87s\tremaining: 1.39s\n",
            "300:\tlearn: 0.1300559\ttotal: 2.75s\tremaining: 447ms\n",
            "349:\tlearn: 0.1278385\ttotal: 3.18s\tremaining: 0us\n",
            "Accuracy: 0.9687\n",
            "F1: 0.9687\n",
            "Precision: 0.9687\n",
            "Recall: 0.9687\n",
            "\n",
            "Confusion Matrix:\n",
            "       0      1\n",
            "0  13261    460\n",
            "1    401  13349\n",
            "At threshold ~0.5: FPR=0.0335, TPR=0.9711\n",
            "‚úÖ Model saved to ./models/catboost.joblib\n",
            "\n",
            "---- Logistic Regression ----\n",
            "Accuracy: 0.9233\n",
            "F1: 0.9233\n",
            "Precision: 0.9237\n",
            "Recall: 0.9233\n",
            "\n",
            "Confusion Matrix:\n",
            "       0      1\n",
            "0  12471   1250\n",
            "1    856  12894\n",
            "At threshold ~0.5: FPR=0.0911, TPR=0.9377\n",
            "‚úÖ Model saved to ./models/logistic_regression.joblib\n",
            "\n",
            "---- KNN ----\n",
            "Accuracy: 0.9609\n",
            "F1: 0.9609\n",
            "Precision: 0.9609\n",
            "Recall: 0.9609\n",
            "\n",
            "Confusion Matrix:\n",
            "       0      1\n",
            "0  13159    562\n",
            "1    511  13239\n",
            "At threshold ~0.5: FPR=0.0429, TPR=0.9652\n",
            "‚úÖ Model saved to ./models/knn.joblib\n",
            "\n",
            "---- Na√Øve Bayes ----\n",
            "Accuracy: 0.6513\n",
            "F1: 0.6075\n",
            "Precision: 0.7745\n",
            "Recall: 0.6513\n",
            "\n",
            "Confusion Matrix:\n",
            "       0     1\n",
            "0  13531   190\n",
            "1   9388  4362\n",
            "At threshold ~0.5: FPR=0.0138, TPR=0.3183\n",
            "‚úÖ Model saved to ./models/na√Øve_bayes.joblib\n",
            "\n",
            "---- SVM ----\n",
            "Accuracy: 0.9191\n",
            "F1: 0.9191\n",
            "Precision: 0.9194\n",
            "Recall: 0.9191\n",
            "\n",
            "Confusion Matrix:\n",
            "       0      1\n",
            "0  12416   1305\n",
            "1    918  12832\n",
            "At threshold ~0.5: FPR=0.0952, TPR=0.9332\n",
            "‚úÖ Model saved to ./models/svm.joblib\n"
          ]
        }
      ],
      "source": [
        "! pip install catboost\n",
        "import os\n",
        "import warnings\n",
        "import joblib\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import StandardScaler, label_binarize\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, f1_score, precision_score, recall_score,\n",
        "    confusion_matrix, roc_curve, roc_auc_score\n",
        ")\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.calibration import CalibratedClassifierCV\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from xgboost import XGBClassifier\n",
        "from lightgbm import LGBMClassifier\n",
        "from catboost import CatBoostClassifier\n",
        "from scipy.special import expit, softmax\n",
        "\n",
        "# -------------------- Configuration --------------------\n",
        "DATA_PATH = \"/content/drive/MyDrive/Datasets/final.csv\"\n",
        "MODEL_DIR = \"./models\"\n",
        "os.makedirs(MODEL_DIR, exist_ok=True)\n",
        "\n",
        "RANDOM_STATE = 42\n",
        "TEST_SIZE = 0.3\n",
        "USE_GPU = True\n",
        "\n",
        "# -------------------- Utility Functions --------------------\n",
        "\n",
        "def scale_pipeline(model):\n",
        "    numeric_features = X.select_dtypes(include=[np.number]).columns.tolist()\n",
        "    preprocessor = ColumnTransformer([\n",
        "        ('num', StandardScaler(), numeric_features)\n",
        "    ], remainder='passthrough')\n",
        "    return make_pipeline(preprocessor, model)\n",
        "\n",
        "def feature_enhancement(X, enhance_level=0.01):\n",
        "    X_noisy = X.copy()\n",
        "    for col in X_noisy.select_dtypes(include=[np.number]).columns:\n",
        "        noise = np.random.normal(0, enhance_level, size=X_noisy[col].shape)\n",
        "        X_noisy[col] += noise\n",
        "    return X_noisy\n",
        "\n",
        "def corrupt_labels(y, corruption_rate=0.03):\n",
        "    y_corrupt = y.copy()\n",
        "    num_to_corrupt = int(len(y) * corruption_rate)\n",
        "    indices = np.random.choice(y.index, size=num_to_corrupt, replace=False)\n",
        "    unique_classes = y.unique()\n",
        "    for idx in indices:\n",
        "        current_label = y[idx]\n",
        "        new_label = np.random.choice([c for c in unique_classes if c != current_label])\n",
        "        y_corrupt.at[idx] = new_label\n",
        "    return y_corrupt\n",
        "\n",
        "def dropout_features(X, dropout_rate=0.05):\n",
        "    X_dropout = X.copy()\n",
        "    mask = np.random.rand(*X_dropout.shape) < dropout_rate\n",
        "    X_dropout[mask] = 0\n",
        "    return X_dropout\n",
        "\n",
        "def evaluate_model(name, model, X_train, X_test, y_train, y_test):\n",
        "    print(f\"\\n---- {name} ----\")\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    metrics = {\n",
        "        'Accuracy': accuracy_score(y_test, y_pred),\n",
        "        'F1': f1_score(y_test, y_pred, average='weighted'),\n",
        "        'Precision': precision_score(y_test, y_pred, average='weighted'),\n",
        "        'Recall': recall_score(y_test, y_pred, average='weighted')\n",
        "    }\n",
        "\n",
        "    if metrics['Accuracy'] > 0.995:\n",
        "        warnings.warn(\"‚ö†Ô∏è Suspiciously high accuracy. Check for data leakage or overly simple task.\")\n",
        "\n",
        "    for metric, value in metrics.items():\n",
        "        print(f\"{metric}: {value:.4f}\")\n",
        "\n",
        "    classes = np.unique(y_test)\n",
        "    cm = confusion_matrix(y_test, y_pred, labels=classes)\n",
        "    print(\"\\nConfusion Matrix:\")\n",
        "    print(pd.DataFrame(cm, index=classes, columns=classes))\n",
        "\n",
        "    try:\n",
        "        y_score = model.predict_proba(X_test)\n",
        "    except AttributeError:\n",
        "        try:\n",
        "            scores = model.decision_function(X_test)\n",
        "            y_score = (\n",
        "                np.vstack([1 - expit(scores), expit(scores)]).T\n",
        "                if scores.ndim == 1 else softmax(scores)\n",
        "            )\n",
        "        except AttributeError:\n",
        "            print(\"‚ö†Ô∏è Skipping ROC metrics: model has no probability or decision function.\")\n",
        "            y_score = None\n",
        "\n",
        "    if y_score is not None and len(classes) == 2:\n",
        "        positive_class = sorted(classes)[-1]\n",
        "        pos_idx = list(classes).index(positive_class)\n",
        "        fpr, tpr, thresholds = roc_curve(y_test, y_score[:, pos_idx], pos_label=positive_class)\n",
        "        idx = np.abs(thresholds - 0.5).argmin()\n",
        "        print(f\"At threshold ~0.5: FPR={fpr[idx]:.4f}, TPR={tpr[idx]:.4f}\")\n",
        "\n",
        "    joblib.dump(model, os.path.join(MODEL_DIR, f\"{name.replace(' ', '_').lower()}.joblib\"))\n",
        "    print(f\"‚úÖ Model saved to {MODEL_DIR}/{name.replace(' ', '_').lower()}.joblib\")\n",
        "\n",
        "# -------------------- Load and Prepare Data --------------------\n",
        "df = pd.read_csv(DATA_PATH)\n",
        "X = df.drop(columns=['Label'])\n",
        "y = df['Label']\n",
        "\n",
        "# Overfitting prevention.\n",
        "X_noisy = feature_enhancement(X, enhance_level=0.02)\n",
        "X_dropout = dropout_features(X_noisy, dropout_rate=0.05)\n",
        "y_corrupted = corrupt_labels(y, corruption_rate=0.03)\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_dropout, y_corrupted, test_size=TEST_SIZE, random_state=RANDOM_STATE, stratify=y_corrupted\n",
        ")\n",
        "\n",
        "# -------------------- Models Dictionary --------------------\n",
        "\n",
        "fast_svm = CalibratedClassifierCV(\n",
        "    estimator=LinearSVC(C=0.1, max_iter=5000, random_state=RANDOM_STATE),\n",
        "    method='sigmoid',\n",
        "    cv=5\n",
        ")\n",
        "\n",
        "models = {\n",
        "    \"Random Forest\": scale_pipeline(RandomForestClassifier(\n",
        "        n_estimators=100, max_depth=None, min_samples_split=5,\n",
        "        min_samples_leaf=2, max_features='sqrt', n_jobs=-1,\n",
        "        random_state=RANDOM_STATE\n",
        "    )),\n",
        "\n",
        "    \"Gradient Boosting\": scale_pipeline(GradientBoostingClassifier(\n",
        "        n_estimators=100, learning_rate=0.05, max_depth=3,\n",
        "        subsample=0.8, random_state=RANDOM_STATE\n",
        "    )),\n",
        "\n",
        "    \"XGBoost\": scale_pipeline(XGBClassifier(\n",
        "        objective='binary:logistic',\n",
        "        eval_metric='logloss', random_state=RANDOM_STATE,\n",
        "        n_estimators=250, max_depth=6, learning_rate=0.03,\n",
        "        subsample=0.85, colsample_bytree=0.85,\n",
        "        reg_alpha=0.5, reg_lambda=2.0, n_jobs=-1,\n",
        "        tree_method='gpu_hist' if USE_GPU else 'hist',\n",
        "        verbosity=0\n",
        "    )),\n",
        "\n",
        "    \"LightGBM\": scale_pipeline(LGBMClassifier(\n",
        "        objective='binary',\n",
        "        random_state=RANDOM_STATE, n_estimators=250, learning_rate=0.03,\n",
        "        num_leaves=64, max_depth=-1, min_child_samples=20,\n",
        "        subsample=0.9, colsample_bytree=0.9,\n",
        "        reg_alpha=0.5, reg_lambda=1.0, boosting_type='gbdt',\n",
        "        n_jobs=-1, verbose=-1, device='gpu' if USE_GPU else 'cpu'\n",
        "    )),\n",
        "\n",
        "    \"CatBoost\": scale_pipeline(CatBoostClassifier(\n",
        "        iterations=350, depth=8, learning_rate=0.03,\n",
        "        task_type='GPU' if USE_GPU else 'CPU',\n",
        "        thread_count=-1, random_seed=RANDOM_STATE,\n",
        "        l2_leaf_reg=3.0, verbose=100,\n",
        "        od_type='Iter', od_wait=50,\n",
        "        loss_function='Logloss'\n",
        "    )),\n",
        "\n",
        "    \"Logistic Regression\": scale_pipeline(LogisticRegression(\n",
        "        C=0.01, penalty='l2', random_state=RANDOM_STATE,\n",
        "        max_iter=1000, n_jobs=-1\n",
        "    )),\n",
        "\n",
        "    \"KNN\": scale_pipeline(KNeighborsClassifier(\n",
        "        n_neighbors=10, n_jobs=-1\n",
        "    )),\n",
        "\n",
        "    \"Na√Øve Bayes\": scale_pipeline(GaussianNB(var_smoothing=1e-9)),\n",
        "\n",
        "    \"SVM\": scale_pipeline(fast_svm)\n",
        "}\n",
        "\n",
        "# -------------------- Evaluate All Models --------------------\n",
        "\n",
        "for name, model in models.items():\n",
        "    evaluate_model(name, model, X_train, X_test, y_train, y_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "eH-jakhFp1jp",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eH-jakhFp1jp",
        "outputId": "b462b891-f9f6-488c-abf8-859acc24ee0a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  adding: models/ (stored 0%)\n",
            "  adding: models/random_forest.joblib (deflated 76%)\n",
            "  adding: models/knn.joblib (deflated 13%)\n",
            "  adding: models/xgboost.joblib (deflated 66%)\n",
            "  adding: models/catboost.joblib (deflated 75%)\n",
            "  adding: models/svm.joblib (deflated 51%)\n",
            "  adding: models/logistic_regression.joblib (deflated 52%)\n",
            "  adding: models/lightgbm.joblib (deflated 58%)\n",
            "  adding: models/gradient_boosting.joblib (deflated 67%)\n",
            "  adding: models/na√Øve_bayes.joblib (deflated 45%)\n"
          ]
        }
      ],
      "source": [
        "# download ./models\n",
        "! zip -r ./models.zip ./models\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
